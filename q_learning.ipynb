{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index'></a>\n",
    "# Q Learning\n",
    "\n",
    "* [Taxi v2 - a deterministic game](#taxi-v2)\n",
    "    * [Training](#taxi-v2-training)\n",
    "    * [Test](#taxi-v2-test)\n",
    "* [FrozenLake - a stochastic game](#frozen-lake)\n",
    "    * [Training](#frozen-lake-training)\n",
    "    * [Test](#frozen-lake-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='taxi-v2'></a>\n",
    "## Taxi v2 - deterministic game\n",
    "\n",
    "There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "\n",
    "See the full [description](https://gym.openai.com/envs/Taxi-v2/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. states: 500\n",
      "No. actions: 6\n"
     ]
    }
   ],
   "source": [
    "# Environment, states, actions, and Q table initialisation\n",
    "\n",
    "env = gym.make('Taxi-v2')\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "print('No. states: {}\\nNo. actions: {}'.format(Q.shape[0], Q.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='taxi-v2-training'></a>\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b45a261a0fb4894b3aa47cbbd9245e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training: building the Q-table\n",
    "def train_q_table(\n",
    "    env: gym.wrappers.time_limit.TimeLimit,\n",
    "    Q: Sequence[Sequence[float]],\n",
    "    n_episodes: int,\n",
    "    n_t_periods: int,\n",
    "    alpha: float=0.7, # learning rate\n",
    "    gamma: float=0.6, # discount rate\n",
    "    epslion_0: float=1.0, # starting exploration rate\n",
    "    epsilon_min: float=0.01, # minimum exploration rate\n",
    "    decay: float=0.01,  # exponential decay rate of epsilon\n",
    "    random_state: int=None\n",
    "):\n",
    "    epsilon = epsilon_0\n",
    "    rng = np.random.RandomState(random_state) \n",
    "    \n",
    "    with tqdm_notebook(total=n_episodes) as pbar:\n",
    "        for ep in range(n_episodes):\n",
    "            epsilon = epsilon_0 * np.exp(-decay * ep) if epsilon > epsilon_min else epsilon_min\n",
    "            s = env.reset() # state\n",
    "            game_over = False\n",
    "\n",
    "            for t in range(n_t_periods):\n",
    "                # Epsilon-Greedy\n",
    "                if rng.rand() > epsilon:\n",
    "                    a = np.argmax(Q[s, :]) # action\n",
    "                else:\n",
    "                    a = env.action_space.sample()\n",
    "\n",
    "                s_new, reward, game_over, info = env.step(a)\n",
    "                Q[s, a] += alpha * (reward + gamma * np.max(Q[s_new, :]) - Q[s, a])\n",
    "                s = s_new\n",
    "\n",
    "                if game_over:\n",
    "                    break\n",
    "\n",
    "            pbar.update(1)\n",
    "            \n",
    "\n",
    "train_q_table(\n",
    "    env,\n",
    "    Q, \n",
    "    n_episodes=50000, \n",
    "    n_t_periods=100, \n",
    "    random_state=42\n",
    ")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='taxi-v2-test'></a>\n",
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d172bdfee4f84016aa44930ae2ca5acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min reward: 3.0\n",
      "Avg. reward: 8.44\n",
      "Max reward: 15.0\n"
     ]
    }
   ],
   "source": [
    "def test_q_table(\n",
    "    env: gym.wrappers.time_limit.TimeLimit,\n",
    "    Q: Sequence[Sequence[float]],\n",
    "    n_episodes,\n",
    "    n_t_periods\n",
    "):\n",
    "    rewards = np.zeros(n_test_episodes)\n",
    "\n",
    "    with tqdm_notebook(total=n_episodes) as pbar:\n",
    "        for ep in range(n_episodes):\n",
    "            s = env.reset()\n",
    "            game_over = False\n",
    "            total = 0\n",
    "\n",
    "            for t in range(n_t_periods):\n",
    "                a = np.argmax(Q[s, :])\n",
    "                s_new, reward, game_over, info = env.step(a)\n",
    "                total += reward\n",
    "                s = s_new\n",
    "\n",
    "                if game_over:\n",
    "                    break\n",
    "\n",
    "            rewards[ep] = total\n",
    "            pbar.update(1)\n",
    "        return rewards\n",
    "    \n",
    "    \n",
    "rewards = test_q_table(env, Q, n_episodes=100, n_t_periods=100)    \n",
    "\n",
    "print('Min reward:', rewards.min())\n",
    "print('Avg. reward:', rewards.mean())\n",
    "print('Max reward:', rewards.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='frozen-lake'></a>\n",
    "## FrozenLake - a stochastic game\n",
    "\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "See the full [description](https://gym.openai.com/envs/FrozenLake-v0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. states: 16\n",
      "No. actions: 4\n"
     ]
    }
   ],
   "source": [
    "# Environment, states, actions, and Q table initialisation\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "print('No. states: {}\\nNo. actions: {}'.format(Q.shape[0], Q.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12b161d70414e99b5eed01bbd691cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_q_table(\n",
    "    env,\n",
    "    Q, \n",
    "    n_episodes=10000, \n",
    "    n_t_periods=100, \n",
    "    random_state=42\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b46ed09cd349c2a616f2d04bc073dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min reward: 0.0\n",
      "Avg. reward: 0.37\n",
      "Max reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "rewards = test_q_table(env, Q, n_episodes=100, n_t_periods=100)    \n",
    "\n",
    "print('Min reward:', rewards.min())\n",
    "print('Avg. reward:', rewards.mean())\n",
    "print('Max reward:', rewards.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
