{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index'></a>\n",
    "# Deep Q Learning\n",
    "\n",
    "Vanilla Deep Q Learning with Epsilon-Greedy implementation.\n",
    "\n",
    "* [Space Invaders](#space-invaders)\n",
    "    * [DQN](#dqn)\n",
    "    * [Memory](#memory)\n",
    "    * [Training](#training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple\n",
    "import warnings\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='space-invaders'></a>\n",
    "## Space Invaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: Box(210, 160, 3)\n",
      "No. actions: 6\n"
     ]
    }
   ],
   "source": [
    "# env = retro.make(game='SpaceInvaders-Atari2600')\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "one_hot_actions = np.array(np.identity(env.action_space.n, dtype=np.int64))\n",
    "\n",
    "print('State size: {}\\nNo. actions: {}'.format(env.observation_space, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(frame, resize=(110, 84)):\n",
    "    '''\n",
    "        Converts the frame from RGB to \n",
    "        grayscale, crops it, normalises \n",
    "        the values to 0-1 range, and re-\n",
    "        sizes to 110 x 84.\n",
    "    '''\n",
    "    gray = rgb2gray(frame)\n",
    "    cropped = gray[8:-12, 4:-12]\n",
    "    normalised = cropped / 255.0\n",
    "    return transform.resize(normalised, resize) if resize else normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stack, frame, resize=(110, 84)):\n",
    "    '''\n",
    "        Adds a frame to the stack\n",
    "        and returns a 3D np.array \n",
    "        of stacked frames, i.e.\n",
    "        110 x 84 x 3\n",
    "    '''\n",
    "    stack.append(frame)\n",
    "    return np.stack(stack, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tensorboard(network, path):\n",
    "    '''\n",
    "        Launch with: tensorboard --logdir=path\n",
    "    '''\n",
    "    writer = tf.summary.FileWriter(path)\n",
    "    tf.summary.scalar('Loss', network.loss)\n",
    "    return writer, tf.summary.merge_all() # tensorboard \"handle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_frames(frames, figsize=(10, 15)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    return animation.FuncAnimation(fig, animate, frames=len(frames), interval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='dqn'></a>\n",
    "### DQN - Deep Q Network\n",
    "\n",
    "A convolutional neural network based DQN model. The input is a stack of frames (one stack is equal to a state) while the output is estimated Q values for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size: Tuple[int, int, int],\n",
    "        n_actions: int,\n",
    "        alpha: int,\n",
    "        name: str='DQN'\n",
    "    ):\n",
    "        self.reset()\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            '''\n",
    "                A context manager for defining ops that creates variables (layers).\n",
    "\n",
    "                This context manager validates that the (optional) values are from t\n",
    "                he same graph, ensures that graph is the default graph, and pushes a \n",
    "                name scope and a variable scope.\n",
    "                \n",
    "                See https://www.tensorflow.org/api_docs/python/tf/variable_scope \n",
    "            '''\n",
    "            self.inputs = tf.placeholder(tf.float32, (None, *state_size), name='inputs')\n",
    "            self.actions = tf.placeholder(tf.float32, (None, n_actions), name='actions')\n",
    "            \n",
    "            # Q_target is considered to be \"true\" Q value but in fact it's estimated with\n",
    "            # R(s, a) + gamma * max(Q_hat(s', a')) where R is a reward \n",
    "            self.Q_target = tf.placeholder(tf.float32, [None], name='Q_target')\n",
    "            \n",
    "            self.__conv1 = tf.layers.conv2d(\n",
    "                inputs=self.inputs,\n",
    "                activation=tf.nn.elu,\n",
    "                filters=32,\n",
    "                kernel_size=(8, 8),\n",
    "                strides=(4, 4),\n",
    "                padding='VALID',\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                name='conv1'\n",
    "            )\n",
    "            \n",
    "            self.__conv2 = tf.layers.conv2d(\n",
    "                inputs=self.__conv1,\n",
    "                activation=tf.nn.elu,\n",
    "                filters=64, \n",
    "                kernel_size=(4, 4),\n",
    "                strides=(2, 2),\n",
    "                padding='VALID',\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                name='conv2'\n",
    "            )\n",
    "            \n",
    "            self.__conv3 = tf.layers.conv2d(\n",
    "                inputs=self.__conv2,\n",
    "                activation=tf.nn.elu,\n",
    "                filters=64, \n",
    "                kernel_size=(3, 3),\n",
    "                strides=(2, 2),\n",
    "                padding='VALID',\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                name='conv3'\n",
    "            )\n",
    "            \n",
    "            self.__dense1 = tf.layers.dense(\n",
    "                inputs=tf.layers.flatten(self.__conv3, name='flattened'),\n",
    "                activation=tf.nn.elu,\n",
    "                units=512,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                name='dense1'\n",
    "            )\n",
    "            \n",
    "            self.__dense2 = tf.layers.dense(\n",
    "                inputs=self.__dense1,\n",
    "                activation=None,\n",
    "                units=n_actions,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                name='dense2'\n",
    "            )\n",
    "            \n",
    "            self.output = self.__dense2\n",
    "            self.Q_hat = tf.reduce_sum(tf.multiply(self.output, self.actions), axis=1) # estimated Q\n",
    "            self.loss = tf.reduce_mean(tf.square(self.Q_target - self.Q_hat))\n",
    "            self.optimiser = tf.train.AdamOptimizer(alpha).minimize(self.loss)\n",
    "            \n",
    "    def reset(self):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    def predict_action(self, session, state):\n",
    "        '''\n",
    "            Predicts the most probable action \n",
    "            for a given, single state.\n",
    "        '''\n",
    "        Q = session.run(\n",
    "            self.output, \n",
    "            feed_dict={\n",
    "                self.inputs: state.reshape((1, *state.shape))\n",
    "            }\n",
    "        )\n",
    "        return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='memory'></a>\n",
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, size: int, random_state: int=None):\n",
    "        self.__rng = np.random.RandomState(random_state)\n",
    "        self.__buffer = deque(maxlen=size)\n",
    "        \n",
    "    def add(self, experience: Tuple):\n",
    "        self.__buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size: int):\n",
    "        assert batch_size < len(self.__buffer)\n",
    "        sample = rng.choice(np.arange(len(self.__buffer)), size=batch_size, replace=False)\n",
    "        return [self.__buffer[i] for i in sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='training'></a>\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_size = (110, 84) # size of the preprocessed game frame\n",
    "stack_size = 4 # no. frames stacked together\n",
    "batch_size = 64\n",
    "n_pre_training_episodes = 128\n",
    "n_training_episodes = 1000\n",
    "n_t_periods = 50000\n",
    "alpha = 1e-3 # learning rate\n",
    "epsilon_0 = 1 # starting exploration rate\n",
    "epsilon_min = 1e-2\n",
    "decay = 1e-3 # exploration decay rate\n",
    "gamma = 0.9 # rewards discount rate\n",
    "memory_size = 1000\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "memory = Memory(memory_size, random_state=42)\n",
    "state_size = (*frame_size, stack_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame = env.reset()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 15), ncols=2)\n",
    "\n",
    "ax[0].imshow(frame)\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('Original')\n",
    "\n",
    "preprocessed = preprocess(frame, resize=frame_size)\n",
    "assert preprocessed.shape == frame_size == np.zeros(frame_size).shape\n",
    "\n",
    "ax[1].imshow(preprocessed, cmap='gray')\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('Preprocessed')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-9-1591094c64f3>:36: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/karolis/miniconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-9-1591094c64f3>:62: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-1591094c64f3>:66: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/karolis/miniconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea16dfaf0494e22b90c1d6924d2ec01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=128), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fef3b0b392e4afab9a7d5b758b79642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-79664aaf51e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mQ_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mQ_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_state_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mgame_over_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 loss = session.run(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_frames = None\n",
    "\n",
    "dqn = DQN(state_size, env.action_space.n, alpha)\n",
    "tensorboard_handle, tensorboard_op = setup_tensorboard(dqn, path='tensorboard/dqn/1')\n",
    "checkpointer = tf.train.Saver()\n",
    "max_total_reward = float('-inf')\n",
    "min_loss = float('inf')\n",
    "rewards = np.zeros(n_training_episodes)\n",
    "epsilon = epsilon_0\n",
    "frame = env.reset()\n",
    "preprocessed = preprocess(frame, resize=frame_size)\n",
    "stack = deque([preprocessed for _ in range(stack_size)], maxlen=stack_size) \n",
    "state = np.stack(stack, axis=2)\n",
    "\n",
    "assert state.shape == state_size\n",
    "assert n_pre_training_episodes > batch_size\n",
    "\n",
    "# \"Pre-train\" the agent by populating the memory\n",
    "# with experiences (state, action, reward, state_new, game_over)\n",
    "# of taking random actions.\n",
    "with tqdm_notebook(total=n_pre_training_episodes) as pbar:\n",
    "    pbar.set_description('Pre-training')\n",
    "    \n",
    "    for ep in range(n_pre_training_episodes):\n",
    "        action = rng.randint(0, env.action_space.n)\n",
    "        frame, reward, game_over, _ = env.step(action)\n",
    "\n",
    "        if game_over:\n",
    "            state_new = stack_frames(stack, np.zeros(frame_size))\n",
    "            memory.add((state, action, reward, state_new, game_over))\n",
    "            \n",
    "            # Reset state\n",
    "            frame = env.reset()\n",
    "            preprocessed = preprocess(frame, resize=frame_size)\n",
    "            stack = deque([preprocessed for _ in range(stack_size)], maxlen=stack_size) \n",
    "            state = np.stack(stack, axis=2)\n",
    "        else:\n",
    "            state_new = stack_frames(stack, preprocess(frame, resize=frame_size))\n",
    "            memory.add((state, action, reward, state_new, game_over))\n",
    "            state = state_new            \n",
    "            \n",
    "        pbar.update(1)\n",
    "\n",
    "with tqdm_notebook(total=n_training_episodes) as pbar:\n",
    "    pbar.set_description('Training')\n",
    "    \n",
    "    with tf.Session() as session:       \n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for ep in range(n_training_episodes):\n",
    "            # Decay exploration rate\n",
    "            epsilon = epsilon_0 * np.exp(-decay * ep) if epsilon > epsilon_min else epsilon_min \n",
    "            \n",
    "            # Reset state\n",
    "            frame = env.reset()\n",
    "            frames = [frame]\n",
    "            preprocessed = preprocess(frame, resize=frame_size)\n",
    "            stack = deque([preprocessed for _ in range(stack_size)], maxlen=stack_size) \n",
    "            state = np.stack(stack, axis=2)\n",
    "            \n",
    "            total_reward = 0\n",
    "            \n",
    "            for t in range(n_t_periods):\n",
    "                '''\n",
    "                    Actual Deep Q Learning part.\n",
    "                '''\n",
    "                # Learning with experience\n",
    "                batch = memory.sample(batch_size) # will retrieves batch_size x states\n",
    "                state_batch = np.array([b[0] for b in batch], ndmin=3) # current\n",
    "                action_batch = np.array([one_hot_actions[b[1]] for b in batch])\n",
    "                reward_batch = np.array([b[2] for b in batch])\n",
    "                state_new_batch = np.array([b[3] for b in batch], ndmin=3) # future\n",
    "                game_over_batch = np.array([b[4] for b in batch])\n",
    "                \n",
    "                # Estimate Q values for the states\n",
    "                Q_state_new = session.run(\n",
    "                    dqn.output, \n",
    "                    feed_dict={\n",
    "                        dqn.inputs: state_new_batch\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Get Q targets\n",
    "                Q_target = np.zeros((batch_size))\n",
    "                for i in range(batch_size):\n",
    "                    Q_target[i] = reward_batch[i] + gamma * np.max(Q_state_new[i]) * (not game_over_batch[i])\n",
    "                \n",
    "                loss = session.run(\n",
    "                    dqn.loss,\n",
    "                    feed_dict={\n",
    "                        dqn.inputs: state_batch,\n",
    "                        dqn.Q_target: Q_target,\n",
    "                        dqn.actions: action_batch\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                if loss < min_loss:\n",
    "                    min_loss = loss\n",
    "                \n",
    "                summary = session.run(\n",
    "                    tensorboard_op,\n",
    "                    feed_dict={\n",
    "                        dqn.inputs: state_batch,\n",
    "                        dqn.Q_target: Q_target,\n",
    "                        dqn.actions: action_batch\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Utils\n",
    "                tensorboard_handle.add_summary(summary)\n",
    "                tensorboard_handle.flush()                \n",
    "                    \n",
    "                '''\n",
    "                    Action prediction.\n",
    "                '''\n",
    "                if rng.rand() > epsilon:\n",
    "                    action = rng.randint(0, env.action_space.n)\n",
    "                else:\n",
    "                    action = dqn.predict_action(session, state)\n",
    "                    \n",
    "                frame, reward, game_over, _ = env.step(action)\n",
    "                frames.append(frame)\n",
    "                preprocessed = preprocess(frame, resize=frame_size)\n",
    "                state_new = stack_frames(stack, preprocessed)\n",
    "                total_reward += reward\n",
    "                \n",
    "                if game_over:\n",
    "                    # Game over: stack an empty frame\n",
    "                    state_new = stack_frames(stack, np.zeros(frame_size))\n",
    "                    memory.add((state, action, reward, state_new, game_over))\n",
    "                    break\n",
    "                \n",
    "                state_new = stack_frames(stack, preprocessed)\n",
    "                memory.add((state, action, reward, state_new, game_over))\n",
    "                state = state_new\n",
    "                \n",
    "            if total_reward > max_total_reward:\n",
    "                max_total_reward = total_reward    \n",
    "                best_frames = frames\n",
    "                checkpointer.save(session, 'model/model')\n",
    "                \n",
    "            rewards[ep] = total_reward\n",
    "                \n",
    "            pbar.set_description('Ep: {}, Current: {}, Best: {}'.format(\n",
    "                ep + 1, total_reward, max_total_reward\n",
    "            ))        \n",
    "            \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.plot(rewards)\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_xlabel('Episode')\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = animate_frames(best_frames)\n",
    "a.save('best_training.mpeg', writer='ffmpeg')\n",
    "HTML(a.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='test'></a>\n",
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.0, mean: 0.0, max: 0.0\n"
     ]
    }
   ],
   "source": [
    "n_test_episodes = 100\n",
    "rewards = np.zeros(n_test_episodes)\n",
    "\n",
    "for ep in range(n_test_episodes):        \n",
    "    total_reward = 0\n",
    "    game_over = False\n",
    "\n",
    "    while not game_over:\n",
    "        action = rng.randint(0, env.action_space.n)\n",
    "        frame_new, reward, game_over, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "    rewards[ep] = total_reward\n",
    "\n",
    "print('Min: {}, mean: {}, max: {}'.format(rewards.min(), rewards.mean(), rewards.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Untrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 5.0, mean: 121.65, max: 595.0\n"
     ]
    }
   ],
   "source": [
    "n_test_episodes = 100\n",
    "dqn = DQN(state_size, env.action_space.n, alpha)\n",
    "rewards = np.zeros(n_test_episodes)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for ep in range(n_test_episodes):        \n",
    "        # Resect state\n",
    "        frame = env.reset()\n",
    "        preprocessed = preprocess(frame_new, resize=frame_size)\n",
    "        stack = deque([preprocessed for _ in range(stack_size)], maxlen=stack_size) \n",
    "        state = np.stack(stack, axis=2)\n",
    "\n",
    "        total_reward = 0\n",
    "        game_over = False\n",
    "\n",
    "        while not game_over:\n",
    "            action = dqn.predict_action(session, state)\n",
    "            frame_new, reward, game_over, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state_new = stack_frames(stack, preprocess(frame_new, resize=frame_size))\n",
    "            state = state_new      \n",
    "        \n",
    "        rewards[ep] = total_reward\n",
    "\n",
    "print('Min: {}, mean: {}, max: {}'.format(rewards.min(), rewards.mean(), rewards.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model\n",
      "Min: 35.0, mean: 247.25, max: 710.0\n"
     ]
    }
   ],
   "source": [
    "n_test_episodes = 100\n",
    "best_frames = None\n",
    "max_total_reward = float('-inf')\n",
    "dqn = DQN(state_size, env.action_space.n, alpha)\n",
    "rewards = np.zeros(n_test_episodes)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    tf.train.import_meta_graph('model/model.meta').restore(session, 'model/model')\n",
    "    \n",
    "    for ep in range(n_test_episodes):        \n",
    "        # Resect state\n",
    "        frame = env.reset()\n",
    "        frames = [frame]\n",
    "        preprocessed = preprocess(frame_new, resize=frame_size)\n",
    "        stack = deque([preprocessed for _ in range(stack_size)], maxlen=stack_size) \n",
    "        state = np.stack(stack, axis=2)\n",
    "\n",
    "        total_reward = 0\n",
    "        game_over = False\n",
    "\n",
    "        while not game_over:\n",
    "            action = dqn.predict_action(session, state)\n",
    "            frame_new, reward, game_over, _ = env.step(action)\n",
    "            frames.append(frame_new)\n",
    "            total_reward += reward\n",
    "            state_new = stack_frames(stack, preprocess(frame_new, resize=frame_size))\n",
    "            state = state_new      \n",
    "        \n",
    "        rewards[ep] = total_reward\n",
    "        \n",
    "        if total_reward > max_total_reward:\n",
    "            max_total_reward = total_reward\n",
    "            best_frames = frames\n",
    "\n",
    "print('Min: {}, mean: {}, max: {}'.format(rewards.min(), rewards.mean(), rewards.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = animate_frames(best_frames)\n",
    "HTML(a.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.save('best_test.mpeg', writer='ffmpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
