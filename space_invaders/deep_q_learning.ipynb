{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index'></a>\n",
    "# Deep Q Learning\n",
    "\n",
    "* [Space Invaders](#space-invaders)\n",
    "    * [DQN](#dqn)\n",
    "    * [Memory](#memory)\n",
    "    * [Training](#training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple\n",
    "import warnings\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='space-invaders'></a>\n",
    "## Space Invaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: Box(210, 160, 3)\n",
      "No. actions: 6\n"
     ]
    }
   ],
   "source": [
    "# env = retro.make(game='SpaceInvaders-Atari2600')\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "one_hot_actions = np.array(np.identity(env.action_space.n, dtype=np.int64))\n",
    "\n",
    "print('State size: {}\\nNo. actions: {}'.format(env.observation_space, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(frame, resize=(110, 84)):\n",
    "    '''\n",
    "        Converts the frame from RGB to \n",
    "        grayscale, crops it, normalises \n",
    "        the values to 0-1 range, and re-\n",
    "        sizes to 110 x 84.\n",
    "    '''\n",
    "    gray = rgb2gray(frame)\n",
    "    cropped = gray[8:-12, 4:-12]\n",
    "    normalised = cropped / 255.0\n",
    "    return transform.resize(normalised, resize) if resize else normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stack, frame, resize=(110, 84)):\n",
    "    '''\n",
    "        Adds a frame to the stack\n",
    "        and returns a 3D np.array \n",
    "        of stacked frames, i.e.\n",
    "        110 x 84 x 3\n",
    "    '''\n",
    "    stack.append(preprocess(frame, resize))\n",
    "    return np.stack(stack, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tensorboard(network, path):\n",
    "    '''\n",
    "        Launch with: tensorboard --logdir=path\n",
    "    '''\n",
    "    writer = tf.summary.FileWriter(path)\n",
    "    tf.summary.scalar('Loss', network.loss)\n",
    "    return writer, tf.summary.merge_all() # tensorboard \"handle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_frames(frames, dpi: int=72):\n",
    "    fig = plt.figure(figsize=(frames[0].shape[1] // dpi, frames[0].shape[0] // dpi), dpi=dpi)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    return animation.FuncAnimation(fi, animate, frames=len(frames), interval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='dqn'></a>\n",
    "### DQN - Deep Q Network\n",
    "\n",
    "A convolutional neural network based DQN model. The input is a stack of frames (one stack is equal to a state) while the output is estimated Q values for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size: Tuple[int, int, int],\n",
    "        n_actions: int,\n",
    "        alpha: int,\n",
    "        name: str='DQN'\n",
    "    ):\n",
    "        self.reset()\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            '''\n",
    "                A context manager for defining ops that creates variables (layers).\n",
    "\n",
    "                This context manager validates that the (optional) values are from t\n",
    "                he same graph, ensures that graph is the default graph, and pushes a \n",
    "                name scope and a variable scope.\n",
    "                \n",
    "                See https://www.tensorflow.org/api_docs/python/tf/variable_scope \n",
    "            '''\n",
    "            self.inputs = tf.placeholder(tf.float32, (None, *state_size), name='inputs')\n",
    "            self.actions = tf.placeholder(tf.float32, (None, n_actions), name='actions')\n",
    "            \n",
    "            # Q_target is considered to be \"true\" Q value but in fact it's estimated with\n",
    "            # R(s, a) + gamma * max(Q_hat(s', a')) where R is a reward \n",
    "            self.Q_target = tf.placeholder(tf.float32, [None], name='Q_target')\n",
    "            \n",
    "            self.__conv1 = tf.layers.conv2d(\n",
    "                inputs=self.inputs,\n",
    "                activation=tf.nn.elu,\n",
    "                filters=32,\n",
    "                kernel_size=(8, 8),\n",
    "                strides=(4, 4),\n",
    "                padding='VALID',\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                name='conv1'\n",
    "            )\n",
    "            \n",
    "            self.__conv2 = tf.layers.conv2d(\n",
    "                inputs=self.__conv1,\n",
    "                activation=tf.nn.elu,\n",
    "                filters=64, \n",
    "                kernel_size=(4, 4),\n",
    "                strides=(2, 2),\n",
    "                padding='VALID',\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                name='conv2'\n",
    "            )\n",
    "            \n",
    "            self.__conv3 = tf.layers.conv2d(\n",
    "                inputs=self.__conv2,\n",
    "                activation=tf.nn.elu,\n",
    "                filters=64, \n",
    "                kernel_size=(3, 3),\n",
    "                strides=(2, 2),\n",
    "                padding='VALID',\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                name='conv3'\n",
    "            )\n",
    "            \n",
    "            self.__dense1 = tf.layers.dense(\n",
    "                inputs=tf.layers.flatten(self.__conv3, name='flattened'),\n",
    "                activation=tf.nn.elu,\n",
    "                units=512,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                name='dense1'\n",
    "            )\n",
    "            \n",
    "            self.__dense2 = tf.layers.dense(\n",
    "                inputs=self.__dense1,\n",
    "                activation=None,\n",
    "                units=n_actions,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                name='dense2'\n",
    "            )\n",
    "            \n",
    "            self.output = self.__dense2\n",
    "            self.Q_hat = tf.reduce_sum(tf.multiply(self.output, self.actions), axis=1) # estimated Q\n",
    "            self.loss = tf.reduce_mean(tf.square(self.Q_target - self.Q_hat), axis=None)\n",
    "            self.optimiser = tf.train.AdamOptimizer(alpha).minimize(self.loss)\n",
    "            \n",
    "    def reset(self):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    def predict_action(self, session, state):\n",
    "        '''\n",
    "            Predicts the most probable action \n",
    "            for a given, single state.\n",
    "        '''\n",
    "        Q = session.run(\n",
    "            self.output, \n",
    "            feed_dict={\n",
    "                self.inputs: state.reshape((1, *state.shape))\n",
    "            }\n",
    "        )\n",
    "        return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='memory'></a>\n",
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, size: int, random_state: int=None):\n",
    "        self.__rng = np.random.RandomState(random_state)\n",
    "        self.__buffer = deque(maxlen=size)\n",
    "        \n",
    "    def add(self, experience: Tuple):\n",
    "        self.__buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size: int):\n",
    "        assert batch_size < len(self.__buffer)\n",
    "        sample = rng.choice(np.arange(len(self.__buffer)), size=batch_size, replace=False)\n",
    "        return [self.__buffer[i] for i in sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='training'></a>\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375f32f175ab42f3a09914dcac2ca173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=128), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83223f7ced82433a92c461b1fdce4027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-be3a61e81145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m                         \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                         \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQ_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                         \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                     }\n\u001b[1;32m     98\u001b[0m                 )\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ML/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frame_size = (110, 84) # size of the preprocessed game frame\n",
    "stack_size = 4 # no. frames stacked together\n",
    "batch_size = 64\n",
    "n_pre_training_episodes = 128\n",
    "n_training_episodes = 50\n",
    "n_t_periods = 50000\n",
    "alpha = 0.001 # learning rate\n",
    "epsilon_0 = 1.0 # starting exploration rate\n",
    "epsilon_min = 0.01\n",
    "decay = 1e-4 # exploration decay rate\n",
    "gamma = 0.9 # rewards discount rate\n",
    "memory_size = 100000\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "frame = env.reset() # get Oth state\n",
    "stack = deque([preprocess(frame) for _ in range(stack_size)], maxlen=stack_size) # setup the frame stack\n",
    "state = stack_frames(stack, frame, resize=frame_size)\n",
    "memory = Memory(memory_size, random_state=42)\n",
    "state_size = (*frame_size, stack_size)\n",
    "epsilon = epsilon_0\n",
    "\n",
    "assert state.shape == state_size\n",
    "assert n_pre_training_episodes > batch_size\n",
    "\n",
    "# \"Pre-train\" the agent by populating the memory\n",
    "# with experiences (state, action, reward, state_new, game_over)\n",
    "# of taking random actions.\n",
    "with tqdm_notebook(total=n_pre_training_episodes) as pbar:\n",
    "    pbar.set_description('Pre-training')\n",
    "    \n",
    "    for ep in range(n_pre_training_episodes):\n",
    "        action = rng.randint(0, env.action_space.n)\n",
    "        frame, reward, game_over, _ = env.step(action)\n",
    "        state_new = stack_frames(stack, frame, resize=frame_size)\n",
    "\n",
    "        if game_over:\n",
    "            memory.add((state, action, reward, np.zeros(state.shape), game_over))\n",
    "            frame = env.reset()\n",
    "            state = stack_frames(stack, frame, resize=frame_size)\n",
    "        else:\n",
    "            memory.add((state, action, reward, state_new, game_over))\n",
    "            state = state_new            \n",
    "            \n",
    "        pbar.update(1)\n",
    "\n",
    "dqn = DQN(state_size, env.action_space.n, alpha)\n",
    "tensorboard_handle, tensorboard_op = setup_tensorboard(dqn, path='tensorboard/dqn/1')\n",
    "checkpointer = tf.train.Saver()\n",
    "max_total_reward = float('-inf')\n",
    "min_loss = float('inf')\n",
    "rewards = np.zeros(n_training_episodes)\n",
    "\n",
    "with tqdm_notebook(total=n_training_episodes) as pbar:\n",
    "    pbar.set_description('Training')\n",
    "    \n",
    "    with tf.Session() as session:       \n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for ep in range(n_training_episodes):\n",
    "            epsilon = epsilon_0 * np.exp(-decay * ep) if epsilon > epsilon_min else epsilon_min\n",
    "            frame = env.reset()\n",
    "            state = stack_frames(stack, frame, resize=frame_size)\n",
    "            game_over = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            for t in range(n_t_periods):\n",
    "                '''\n",
    "                    Actual Deep Q Learning part.\n",
    "                '''\n",
    "                # Learning with experience\n",
    "                batch = memory.sample(batch_size) # will retrieves batch_size x states\n",
    "                state_batch = np.array([b[0] for b in batch], ndmin=3) # current\n",
    "                action_batch = np.array([one_hot_actions[b[1]] for b in batch])\n",
    "                reward_batch = np.array([b[2] for b in batch])\n",
    "                state_new_batch = np.array([b[3] for b in batch], ndmin=3) # future\n",
    "                game_over_batch = np.array([b[4] for b in batch])\n",
    "                \n",
    "                # Estimate Q values for the states\n",
    "                Q_state_new = session.run(\n",
    "                    dqn.output, \n",
    "                    feed_dict={\n",
    "                        dqn.inputs: state_new_batch\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Get Q targets\n",
    "                Q_target = np.zeros((batch_size))\n",
    "                for i in range(batch_size):\n",
    "                    Q_target[i] = reward_batch[i] + gamma * np.max(Q_state_new[i]) * (not game_over_batch[i])\n",
    "                \n",
    "                loss = session.run(\n",
    "                    dqn.loss,\n",
    "                    feed_dict={\n",
    "                        dqn.inputs: state_batch,\n",
    "                        dqn.Q_target: Q_target,\n",
    "                        dqn.actions: action_batch\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                if loss < min_loss:\n",
    "                    min_loss = loss\n",
    "                \n",
    "                summary = session.run(\n",
    "                    tensorboard_op,\n",
    "                    feed_dict={\n",
    "                        dqn.inputs: state_batch,\n",
    "                        dqn.Q_target: Q_target,\n",
    "                        dqn.actions: action_batch\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Utils\n",
    "                tensorboard_handle.add_summary(summary)\n",
    "                tensorboard_handle.flush()                \n",
    "                    \n",
    "                '''\n",
    "                    Action prediction.\n",
    "                '''\n",
    "                if rng.rand() > epsilon:\n",
    "                    action = rng.randint(0, env.action_space.n)\n",
    "                else:\n",
    "                    action = dqn.predict_action(session, state)\n",
    "                    \n",
    "                frame, reward, game_over, _ = env.step(action)\n",
    "                state_new = stack_frames(stack, frame, resize=frame_size)\n",
    "                total_reward += reward\n",
    "                \n",
    "                if game_over:\n",
    "                    # Game over: stack an empty frame\n",
    "                    state_new = stack_frames(stack, np.zeros(frame_size), resize=frame_size)\n",
    "                    memory.add((state, action, reward, state_new, game_over))\n",
    "                    break\n",
    "                else:\n",
    "                    state_new = stack_frames(stack, frame, resize=frame_size)\n",
    "                    memory.add((state, action, reward, state_new, game_over))\n",
    "                    state = state_new\n",
    "                \n",
    "            if total_reward > max_total_reward:\n",
    "                max_total_reward = total_reward           \n",
    "                checkpointer.save(session, 'model/model')\n",
    "                \n",
    "            rewards[ep] = total_reward\n",
    "                \n",
    "            pbar.set_description('Ep: {}, Current: {}, Best: {}'.format(\n",
    "                ep + 1, total_reward, max_total_reward\n",
    "            ))        \n",
    "            \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='test'></a>\n",
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
