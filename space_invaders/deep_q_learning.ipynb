{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='index'></a>\n",
    "# Deep Q Learning\n",
    "\n",
    "* [Space Invaders](#space-invaders)\n",
    "    * [DQN](#dqn)\n",
    "    * [Memory](#memory)\n",
    "    * [Training](#training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple\n",
    "import warnings\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='space-invaders'></a>\n",
    "## Space Invaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size: Box(210, 160, 3)\n",
      "No. actions: 6\n"
     ]
    }
   ],
   "source": [
    "# env = retro.make(game='SpaceInvaders-Atari2600')\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "print('State size: {}\\nNo. actions: {}'.format(env.observation_space, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(frame, resize=(110, 84)):\n",
    "    '''\n",
    "        Converts the frame from RGB to \n",
    "        grayscale, crops it, normalises \n",
    "        the values to 0-1 range, and re-\n",
    "        sizes to 110 x 84.\n",
    "    '''\n",
    "    gray = rgb2gray(frame)\n",
    "    cropped = gray[8:-12, 4:-12]\n",
    "    normalised = cropped / 255.0\n",
    "    return transform.resize(normalised, resize) if resize else normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stack, frame, resize=(110, 84)):\n",
    "    '''\n",
    "        Adds a frame to the stack\n",
    "        and returns a 3D np.array \n",
    "        of stacked frames, i.e.\n",
    "        110 x 84 x 3\n",
    "    '''\n",
    "    stack.append(preprocess(frame, resize))\n",
    "    return np.stack(stack, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tensorboard(network, path):\n",
    "    '''\n",
    "        Launch with: tensorboard --logdir=path\n",
    "    '''\n",
    "    writer = tf.summary.FileWriter(path)\n",
    "    tf.summary.scalar('Loss', network.loss)\n",
    "    return writer, tf.summary.merge_all() # tensorboard \"handle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_frames(frames, dpi: int=72):\n",
    "    fig = plt.figure(figsize=(frames[0].shape[1] // dpi, frames[0].shape[0] // dpi), dpi=dpi)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    return animation.FuncAnimation(fi, animate, frames=len(frames), interval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='dqn'></a>\n",
    "### DQN - Deep Q Network\n",
    "\n",
    "A convolutional neural network based DQN model. The input is a stack of frames (one stack is equal to a state) while the output is estimated Q values for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size: Tuple[int, int, int],\n",
    "        n_actions: int,\n",
    "        alpha: int,\n",
    "        name: str='DQN'\n",
    "    ):\n",
    "        self.reset()\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            '''\n",
    "                A context manager for defining ops that creates variables (layers).\n",
    "\n",
    "                This context manager validates that the (optional) values are from t\n",
    "                he same graph, ensures that graph is the default graph, and pushes a \n",
    "                name scope and a variable scope.\n",
    "                \n",
    "                See https://www.tensorflow.org/api_docs/python/tf/variable_scope \n",
    "            '''\n",
    "            self.inputs = tf.placeholder(tf.float32, (None, *state_size), name='inputs')\n",
    "            self.actions = tf.placeholder(tf.float32, (None, n_actions), name='actions')\n",
    "            \n",
    "            # Q_target is considered to be \"true\" Q value but in fact it's estimated with\n",
    "            # R(s, a) + gamma * max(Q_hat(s', a')) where R is a reward \n",
    "            self.Q_target = tf.placeholder(tf.float32, [None], name='Q_target')\n",
    "            \n",
    "            self.__conv1 = tf.layers.conv2d(\n",
    "                inputs=self.inputs,\n",
    "                activation=tf.nn.elu,\n",
    "                filters=32,\n",
    "                kernel_size=(8, 8),\n",
    "                strides=(4, 4),\n",
    "                padding='VALID',\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                name='conv1'\n",
    "            )\n",
    "            \n",
    "            self.__conv2 = tf.layers.conv2d(\n",
    "                inputs=self.__conv1,\n",
    "                activation=tf.nn.elu,\n",
    "                filters=64, \n",
    "                kernel_size=(4, 4),\n",
    "                strides=(2, 2),\n",
    "                padding='VALID',\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                name='conv2'\n",
    "            )\n",
    "            \n",
    "            self.__conv3 = tf.layers.conv2d(\n",
    "                inputs=self.__conv2,\n",
    "                activation=tf.nn.elu,\n",
    "                filters=64, \n",
    "                kernel_size=(3, 3),\n",
    "                strides=(2, 2),\n",
    "                padding='VALID',\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                name='conv3'\n",
    "            )\n",
    "            \n",
    "            self.__dense1 = tf.layers.dense(\n",
    "                inputs=tf.layers.flatten(self.__conv3, name='flattened'),\n",
    "                activation=tf.nn.elu,\n",
    "                units=512,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                name='dense1'\n",
    "            )\n",
    "            \n",
    "            self.__dense2 = tf.layers.dense(\n",
    "                inputs=self.__dense1,\n",
    "                activation=None,\n",
    "                units=n_actions,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                name='dense2'\n",
    "            )\n",
    "            \n",
    "            self.output = self.__dense2\n",
    "            self.Q_hat = tf.reduce_sum(tf.multiply(self.output, self.actions)) # estimated Q\n",
    "            self.loss = tf.reduce_mean(tf.square(self.Q_target - self.Q_hat))\n",
    "            self.optimiser = tf.train.AdamOptimizer(alpha).minimize(self.loss)\n",
    "            \n",
    "    def reset(self):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    def predict_action(self, session, state):\n",
    "        '''\n",
    "            Predicts the most probable action \n",
    "            for a given, single state.\n",
    "        '''\n",
    "        Q = session.run(\n",
    "            self.output, \n",
    "            feed_dict={\n",
    "                self.inputs: state.reshape((1, *state.shape))\n",
    "            }\n",
    "        )\n",
    "        return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='memory'></a>\n",
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, size: int, random_state: int=None):\n",
    "        self.__rng = np.random.RandomState(random_state)\n",
    "        self.__buffer = deque(maxlen=size)\n",
    "        \n",
    "    def add(self, experience: Tuple):\n",
    "        self.__buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size: int):\n",
    "        assert batch_size < len(self.__buffer)\n",
    "        return self.__rng.choice(self.__buffer, batch_size, replace=False).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='training'></a>\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4487ea70a8dc4aaab70f6029d7118fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc320bbb18c493491c13c0ebf7b290c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "frame_size = (110, 84) # size of the preprocessed game frame\n",
    "stack_size = 4 # no. frames stacked together\n",
    "batch_size = 64\n",
    "n_pre_training_episodes = 128\n",
    "n_training_episodes = 50\n",
    "n_t_periods = 50000\n",
    "alpha = 0.001 # learning rate\n",
    "epsilon_0 = 1.0 # starting exploration rate\n",
    "epsilon_min = 0.01\n",
    "decay = 1e-4 # exploration decay rate\n",
    "gamma = 0.9 # rewards discount rate\n",
    "memory_size = 100000\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "frame = env.reset() # get Oth state\n",
    "stack = deque([preprocess(frame) for _ in range(stack_size)], maxlen=stack_size) # setup the frame stack\n",
    "state = stack_frames(stack, frame, resize=frame_size)\n",
    "memory = Memory(memory_size, random_state=42)\n",
    "state_size = (*frame_size, stack_size)\n",
    "epsilon = epsilon_0\n",
    "\n",
    "assert state.shape == state_size\n",
    "assert n_pre_training_episodes > batch_size\n",
    "\n",
    "# \"Pre-train\" the agent by populating the memory\n",
    "# with experiences (state, action, reward, state_new)\n",
    "# from taking random actions.\n",
    "with tqdm_notebook(total=n_pre_training_episodes) as pbar:\n",
    "    pbar.set_description('Pre-training')\n",
    "    \n",
    "    for ep in range(n_pre_training_episodes):\n",
    "        action = rng.randint(0, env.action_space.n)\n",
    "        frame, reward, game_over, _ = env.step(action)\n",
    "        state_new = stack_frames(stack, frame, resize=frame_size)\n",
    "\n",
    "        if game_over:\n",
    "            state_new = np.zeros(state.shape)\n",
    "            memory.add((state, action, reward, state_new, game_over))\n",
    "            frame = env.reset()\n",
    "            state = stack_frames(stack, frame, resize=frame_size)\n",
    "        else:\n",
    "            memory.add((state, action, reward, state_new, game_over))\n",
    "            state = state_new            \n",
    "            \n",
    "        pbar.update(1)\n",
    "\n",
    "dqn = DQN(state_size, env.action_space.n, alpha)\n",
    "tensorboard_hanlde, tensorboard_op = setup_tensorboard(dqn, path='tensorboard/dqn/1')\n",
    "checkpointer = tf.train.Saver()\n",
    "max_total_reward = float('-inf')\n",
    "min_loss = float('inf')\n",
    "rewards = np.zeros(n_training_episodes)\n",
    "\n",
    "with tqdm_notebook(total=n_training_episodes) as pbar:\n",
    "    pbar.set_description('Training')\n",
    "    \n",
    "    with tf.Session() as session:       \n",
    "        for ep in range(n_training_episodes):\n",
    "            epsilon = epsilon_0 * np.exp(-decay * ep) if epsilon > epsilon_min else epsilon_min\n",
    "            frame = env.reset()\n",
    "            state = stack_frames(stack, frame, resize=frame_size)\n",
    "            game_over = False\n",
    "            total_reward = 0\n",
    "            \n",
    "            for t in range(n_t_periods):\n",
    "                '''\n",
    "                    Actual Deep Q Learning part.\n",
    "                '''\n",
    "                # Learning with experience\n",
    "                batch = memory.sample(batch_size)\n",
    "                state_batch = np.array([b[0] for b in batch], ndmin=3)\n",
    "                action_batch = np.array([b[1] for b in batch])\n",
    "                reward_batch = np.array([b[2] for b in batch])\n",
    "                state_new_batch = np.array([b[3] for b in batch], ndim=3)\n",
    "                game_over_batch = np.array([b[4] for b in batch])\n",
    "                \n",
    "                # Estimate Q values for the states\n",
    "                Q_state_new = session.run(\n",
    "                    dqn.output, \n",
    "                    fead_dict={\n",
    "                        dqn.inputs: state_new_batch\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Get Q targets\n",
    "                for i in range(batch_size):\n",
    "                    Q_target[i] = reward_batch[i] + gamma * np.max(Q_state_new[i]) * ( not game_over_batch[i])\n",
    "                \n",
    "                loss, _ session.run(\n",
    "                    [dqn.output, dqn.optimiser],\n",
    "                    feed_dict={\n",
    "                        dqn.inputs: state_batch,\n",
    "                        dqn.Q_target: Q_target,\n",
    "                        dqn.actions: action_batch\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                loss < min_loss:\n",
    "                    min_loss = loss\n",
    "                \n",
    "                summary = session.run(\n",
    "                    tensorboard_op,\n",
    "                    feed_dict={\n",
    "                        dqn.inputs: state_batch,\n",
    "                        dqn.Q_target: Q_target,\n",
    "                        dqn.actions: action_batch\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Utils\n",
    "                tensorboard_handle.add_summary(summary)\n",
    "                tensorboard_handle.flush()                \n",
    "                    \n",
    "                '''\n",
    "                    Action prediction.\n",
    "                '''\n",
    "                if rng.rand() > epsilon:\n",
    "                    action = rng.randint(0, env.action_space.n)\n",
    "                else:\n",
    "                    action = dqn.predict_action(session, state)\n",
    "                    \n",
    "                frame, reward, game_over, _ = env.step(action)\n",
    "                state_new = stack_frames(stack, frame, resize=frame_size)\n",
    "                total_reward += reward\n",
    "                \n",
    "                if game_over:\n",
    "                    # Game over: stack an empty frame\n",
    "                    state_new = stack_frames(stack, np.zeros(frame_size), resize=None)\n",
    "                    memory.add((state, action, reward, state_new, game_over))\n",
    "                    break\n",
    "                else:\n",
    "                    state_new = stack_frames(stack, frame, resize=frame_size)\n",
    "                    memory.add((state, action, reward, state_new, game_over))\n",
    "                    state = state_new\n",
    "                \n",
    "            if total_reward > max_total_reward:\n",
    "                max_total_reward = total_reward           \n",
    "                checkpointer.save(session, 'model')\n",
    "                \n",
    "            pbar.set_description('Ep: {}, Current t. reward: {}, Best t. reward: {}, min loss: {} '.format(\n",
    "                ep + 1, total_reward, max_total_reward, min_loss\n",
    "            ))        \n",
    "            \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to index](#index)\n",
    "\n",
    "<a id='test'></a>\n",
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
